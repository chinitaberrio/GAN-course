{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_OhgbCztLdb0"
   },
   "source": [
    "### Lecture 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_cG2mZfrLdb2"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "sD4OK2rhLdb7",
    "outputId": "27565a91-e1d6-445a-9079-3642f96ef207"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5xhpncOELdcA",
    "outputId": "b7c46087-327a-492a-9556-0b1f3ba9d0d3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xX2Y2NDhLdcD"
   },
   "outputs": [],
   "source": [
    "noise_dim = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y4wk9P5mLdcG"
   },
   "source": [
    "#### Generator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G_vE7g7KLdcH"
   },
   "outputs": [],
   "source": [
    "class Generator(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.fcn = torch.nn.Sequential(\n",
    "            # Fully Connected Layer 1\n",
    "            torch.nn.Linear(\n",
    "                in_features=noise_dim,\n",
    "                out_features=240,\n",
    "                bias=True\n",
    "            ),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.5),\n",
    "            # Fully Connected Layer 2\n",
    "            torch.nn.Linear(\n",
    "                in_features=240,\n",
    "                out_features=240,\n",
    "                bias=True\n",
    "            ),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.5),\n",
    "            # Fully Connected Layer 3\n",
    "            torch.nn.Linear(\n",
    "                in_features=240,\n",
    "                out_features=240,\n",
    "                bias=True\n",
    "            ),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.5),\n",
    "            # Fully Connected Layer 4\n",
    "            torch.nn.Linear(\n",
    "                in_features=240,\n",
    "                out_features=784,\n",
    "                bias=True\n",
    "            ),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, batch):\n",
    "        ret = batch.view(batch.size(0), -1)\n",
    "        ret = self.fcn(ret)\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DlA1jjXsLdcK"
   },
   "source": [
    "#### Maxout Activation\n",
    "\n",
    "##### Source: https://github.com/pytorch/pytorch/issues/805"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "18YizRRJLdcL"
   },
   "outputs": [],
   "source": [
    "class Maxout(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_pieces):\n",
    "\n",
    "        super(Maxout, self).__init__()\n",
    "\n",
    "        self.num_pieces = num_pieces\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # x.shape = (batch_size? x 625)\n",
    "\n",
    "        assert x.shape[1] % self.num_pieces == 0  # 625 % 5 = 0\n",
    "\n",
    "        ret = x.view(\n",
    "            *x.shape[:1],  # batch_size\n",
    "            x.shape[1] // self.num_pieces,  # piece-wise linear\n",
    "            self.num_pieces,  # num_pieces\n",
    "            *x.shape[2:]  # remaining dimensions if any\n",
    "        )\n",
    "        \n",
    "        # ret.shape = (batch_size? x 125 x 5)\n",
    "\n",
    "        # https://pytorch.org/docs/stable/torch.html#torch.max        \n",
    "        ret, _ = ret.max(dim=2)\n",
    "\n",
    "        # ret.shape = (batch_size? x 125)\n",
    "\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PWHOoCrHLdcO"
   },
   "source": [
    "#### Discriminator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VOU4-L1bLdcP"
   },
   "outputs": [],
   "source": [
    "class Discriminator(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.fcn = torch.nn.Sequential(\n",
    "            # Fully Connected Layer 1\n",
    "            torch.nn.Linear(\n",
    "                in_features=784,\n",
    "                out_features=240,\n",
    "                bias=True\n",
    "            ),\n",
    "            Maxout(5),\n",
    "            # Fully Connected Layer 2\n",
    "            torch.nn.Linear(\n",
    "                in_features=48,\n",
    "                out_features=240,\n",
    "                bias=True\n",
    "            ),\n",
    "            Maxout(5),\n",
    "            # Fully Connected Layer 3\n",
    "            torch.nn.Linear(\n",
    "                in_features=48,\n",
    "                out_features=1,\n",
    "                bias=True\n",
    "            ),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, batch):\n",
    "        ret = batch.view(batch.size(0), -1)\n",
    "        ret = self.fcn(ret)\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Avus67kLdcT"
   },
   "source": [
    "### Lecture 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5uqxuAHtLdcU"
   },
   "source": [
    "#### MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zuyEivYqLdcV"
   },
   "outputs": [],
   "source": [
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "rj1UdgKdLdcX",
    "outputId": "8c6ccf7f-d1d3-43b4-c6b4-4461f8177fda"
   },
   "outputs": [],
   "source": [
    "class FlattenTransform:\n",
    "    \n",
    "    def __call__(self, inputs):\n",
    "        return inputs.view(inputs.shape[0], -1)\n",
    "        \n",
    "\n",
    "data_train = torchvision.datasets.MNIST(\n",
    "    './data/mnist',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        FlattenTransform()\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X0-vutLXLdca"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    data_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RK8UuddZLdcd"
   },
   "source": [
    "#### GAN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Wgg5wzSLdce"
   },
   "outputs": [],
   "source": [
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "\n",
    "discriminator_optimizer = torch.optim.SGD(\n",
    "    discriminator.parameters(),\n",
    "    lr=0.005,\n",
    "    momentum=0.7\n",
    ")\n",
    "\n",
    "generator_optimizer = torch.optim.SGD(\n",
    "    generator.parameters(),\n",
    "    lr=0.005,\n",
    "    momentum=0.7\n",
    ")\n",
    "\n",
    "criterion = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L99QRpJnLdch"
   },
   "outputs": [],
   "source": [
    "real_labels = torch.ones(BATCH_SIZE, 1).to(device)\n",
    "fake_labels = torch.zeros(BATCH_SIZE, 1).to(device)\n",
    "\n",
    "test_set = torch.randn(16, noise_dim).to(device)\n",
    "\n",
    "num_epochs = 512\n",
    "num_steps = len(train_loader) // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bRPDeJpHLdcl"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists('visuals-v2'):\n",
    "    os.mkdir('visuals-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O29i3nMSLdco"
   },
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "\n",
    "    for i, (images, _) in enumerate(train_loader):\n",
    "\n",
    "        if i == num_steps:\n",
    "            break\n",
    "\n",
    "        # Train Discriminator\n",
    "        for _ in range(16):\n",
    "        \n",
    "            real_images = images.to(device)\n",
    "\n",
    "            fake_images = generator(\n",
    "                torch.randn(BATCH_SIZE, noise_dim).to(device)\n",
    "            )\n",
    "\n",
    "            discriminator_optimizer.zero_grad()\n",
    "\n",
    "            real_outputs = discriminator(real_images)\n",
    "            fake_outputs = discriminator(fake_images)\n",
    "\n",
    "            d_x = criterion(real_outputs, real_labels)\n",
    "            d_g_z = criterion(fake_outputs, fake_labels)\n",
    "\n",
    "            d_x.backward()\n",
    "            d_g_z.backward()\n",
    "\n",
    "            discriminator_optimizer.step()\n",
    "\n",
    "        # Train Generator\n",
    "        z = torch.randn(BATCH_SIZE, noise_dim).to(device)\n",
    "\n",
    "        generator.zero_grad()\n",
    "\n",
    "        outputs = discriminator(generator(z))\n",
    "\n",
    "        loss = criterion(outputs, real_labels)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        generator_optimizer.step()\n",
    "\n",
    "    # Visualize Results\n",
    "    if epoch % 10 == 0:\n",
    "\n",
    "        generated = generator(test_set).detach().cpu().view(-1, 1, 28, 28)\n",
    "\n",
    "        grid = torchvision.utils.save_image(\n",
    "            generated,\n",
    "            os.path.join(\n",
    "                'visuals-v2',\n",
    "                '{}.jpg'.format(\n",
    "                    str(epoch).zfill(6)\n",
    "                )\n",
    "            ),\n",
    "            nrow=4,\n",
    "            padding=10,\n",
    "            pad_value=1\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 901
    },
    "colab_type": "code",
    "id": "VxA3WipWLdcr",
    "outputId": "7bcf55fe-e764-4419-f531-32088d1acf2b"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x1152 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize Results\n",
    "generated = generator(test_set).detach().cpu().view(-1, 1, 28, 28)\n",
    "\n",
    "grid = torchvision.utils.make_grid(\n",
    "    generated,\n",
    "    nrow=4,\n",
    "    padding=10,\n",
    "    pad_value=1\n",
    ")\n",
    "\n",
    "img = np.transpose(\n",
    "    grid.numpy(),\n",
    "    (1, 2, 0)\n",
    ")\n",
    "\n",
    "fig = plt.figure(figsize=(16, 16))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(img);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xrah75GiLdcu"
   },
   "source": [
    "#### Google Collaboratory\n",
    "\n",
    "Notebook: https://colab.research.google.com/drive/1ZFvYoDhrLrUwrv-hUpNXJGxndEeRvl8H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oTFhuj-ZNpxM"
   },
   "source": [
    "#### Evaluation Metrics: Inception Score\n",
    "\n",
    "\n",
    "Source: https://sudomake.ai/inception-score-explained/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Iglap_ZqLdcv"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchvision.models import inception_v3\n",
    "\n",
    "# initialize pretrained inception network\n",
    "net = inception_v3(pretrained=True).to(device)\n",
    "\n",
    "\n",
    "def inception_score(images, batch_size):\n",
    "\n",
    "  # list of scores\n",
    "  scores = []\n",
    "\n",
    "  # number of steps\n",
    "  num_steps = int(math.ceil(float(len(images)) / float(batch_size)))\n",
    "\n",
    "  # iterate over the images\n",
    "  for i in range(num_steps):\n",
    "\n",
    "    # mini-batch start and end index\n",
    "    s = i * batch_size\n",
    "    e = (i + 1) * batch_size\n",
    "\n",
    "    # mini-batch images\n",
    "    mini_batch = images[s:e]\n",
    "\n",
    "    # mini-batch as Torch tensor with gradients\n",
    "    batch = Variable(mini_batch)\n",
    "\n",
    "    # apply a forward pass through inception network\n",
    "    # skipping aux logits\n",
    "    '''\n",
    "     This network is unique because it has two output layers when training.\n",
    "     The second output is known as an auxiliary output and is contained in the AuxLogits part of the network.\n",
    "     The primary output is a linear layer at the end of the network.\n",
    "     Note, when testing we only consider the primary output.\n",
    "    '''\n",
    "    s, _ = net(batch)\n",
    "\n",
    "    # accumulate scores\n",
    "    scores.append(s)\n",
    "\n",
    "  # stack scores as tensor\n",
    "  scores = torch.cat(scores, 0)\n",
    "\n",
    "  # calculate inception score\n",
    "\n",
    "  '''\n",
    "  The formula for inception score\n",
    "  IS(x) = E[ KL( P(y|x) || P(y)) ]\n",
    "  x: generated images\n",
    "  y: inception model classification distribution aka softmax\n",
    "  '''\n",
    "\n",
    "  # calculate p(y|x) across dimension 1\n",
    "  # that is one row for each image\n",
    "  p_yx = F.softmax(scores, 1)\n",
    "\n",
    "  # calculate p(y) across dimension 0\n",
    "  # that is one column for each class / label\n",
    "  p_y = p_yx.mean(0).unsqueeze(0).expand(p_yx.size(0), -1)\n",
    "\n",
    "  # calculate KL divergence\n",
    "  KL_d = p_yx * (torch.log(p_yx) - torch.log(p_y))\n",
    "\n",
    "  # calculate mean aka expected of KL\n",
    "  final_score = KL_d.mean()\n",
    "\n",
    "  # return final score\n",
    "  return final_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ZRWS6QXxNojy",
    "outputId": "46b13669-faf9-4d3c-ba3a-9e02fe2f0ea9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0004470464482437819"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test inception score on generated images\n",
    "\n",
    "# generate images\n",
    "images = generator(test_set)\n",
    "\n",
    "# reshape to 1x28x28\n",
    "images = images.view(-1, 1, 28, 28)\n",
    "\n",
    "# repeat Gray channel to RGB\n",
    "images = images.repeat(1, 3, 1, 1)\n",
    "\n",
    "# resize the images to 3x299x299\n",
    "res_images = F.interpolate(images, size=(299, 299))\n",
    "\n",
    "# calculate inception score\n",
    "score = inception_score(res_images, BATCH_SIZE)\n",
    "\n",
    "# get value from tensor variable\n",
    "score.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "eB9H_kK2Swv7",
    "outputId": "94941fa3-4c4c-4e04-cbad-34466d733e3a"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5fb6c9523900>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# load mini batch from dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# convert to GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "# test inception score on real images\n",
    "\n",
    "# load mini batch from dataset\n",
    "images, _ = next(iter(train_loader))\n",
    "\n",
    "# convert to GPU\n",
    "images = images.to(device)\n",
    "\n",
    "# reshape to 1x28x28\n",
    "images = images.view(-1, 1, 28, 28)\n",
    "\n",
    "# repeat Gray channel to RGB\n",
    "images = images.repeat(1, 3, 1, 1)\n",
    "\n",
    "# resize the images to 3x299x299\n",
    "res_images = F.interpolate(images, size=(299, 299))\n",
    "\n",
    "# calculate inception score\n",
    "score = inception_score(res_images, BATCH_SIZE)\n",
    "\n",
    "# get value from tensor variable\n",
    "score.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N91JMwuWbNgi"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "[Coding Exercise] GAN Evaluation Metrics: Inception Score",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
